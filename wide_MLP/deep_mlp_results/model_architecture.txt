Deep MLP Model Architecture\n==================================================\nInput Size: 1024\nHidden Layers: 5\nHidden Sizes: [2048, 2048, 2048, 2048, 2048]\nTotal Parameters: 18,907,137\nTrainable Parameters: 18,907,137\nDropout Rate: 0.3\nBatch Normalization: True\n\nDeepMLP(
  (network): Sequential(
    (0): Linear(in_features=1024, out_features=2048, bias=True)
    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.3, inplace=False)
    (4): Linear(in_features=2048, out_features=2048, bias=True)
    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.1)
    (7): Dropout(p=0.3, inplace=False)
    (8): Linear(in_features=2048, out_features=2048, bias=True)
    (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU()
    (11): Dropout(p=0.3, inplace=False)
    (12): Linear(in_features=2048, out_features=2048, bias=True)
    (13): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): LeakyReLU(negative_slope=0.1)
    (15): Dropout(p=0.3, inplace=False)
    (16): Linear(in_features=2048, out_features=2048, bias=True)
    (17): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): ReLU()
    (19): Dropout(p=0.3, inplace=False)
    (20): Linear(in_features=2048, out_features=1, bias=True)
  )
)